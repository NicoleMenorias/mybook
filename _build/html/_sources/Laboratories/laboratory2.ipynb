{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3f40949",
   "metadata": {},
   "source": [
    "# **Laboratory Task 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8236f1",
   "metadata": {},
   "source": [
    "<style>\n",
    "/* put your CSS here (this cell must be Markdown) */\n",
    ".a-card {\n",
    "  background: linear-gradient(180deg,#f8fff8,#f0fbff);\n",
    "  border-left: 6px solid #2b8cbe;\n",
    "  padding: 18px 22px;\n",
    "  border-radius: 10px;\n",
    "  box-shadow: 0 6px 20px rgba(0,0,0,0.06);\n",
    "  font-family: \"Segoe UI\", Roboto, Arial, sans-serif;\n",
    "  color: #073642;\n",
    "  max-width: 900px;\n",
    "}\n",
    ".a-card h1 { color:#05445E; }\n",
    ".math-block { font-family: \"Times New Roman\", Georgia, serif; padding:8px 0; }\n",
    ".result { display:inline-block; background:#e6fffa; padding:6px 10px; border-radius:6px; font-weight:700; }\n",
    ".mono { font-family: \"SFMono-Regular\", Menlo, Monaco, monospace; }\n",
    "</style>\n",
    "<div class=\"a-card\">\n",
    "\n",
    "**Instruction:** Perform a single forward pass and compute the error.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Given**\n",
    "\n",
    "- Input vector:  \n",
    "  $$x = \\begin{bmatrix}1\\\\[4pt]0\\\\[4pt]1\\end{bmatrix}$$\n",
    "\n",
    "- Target: $$y = 1$$\n",
    "\n",
    "- Hidden activation: ReLU, $$f(z)=\\max(0,z)$$\n",
    "\n",
    "- Hidden weights (3 inputs â†’ 2 hidden units):\n",
    "  $$\n",
    "  W_{\\text{hidden}}=\n",
    "  \\begin{bmatrix}\n",
    "  0.2 & -0.3\\\\[4pt]\n",
    "  0.4 & 0.1\\\\[4pt]\n",
    "  -0.5 & 0.2\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "- Output weights:\n",
    "  $$W_{\\text{out}}=\\begin{bmatrix}-0.3\\\\[4pt]-0.2\\end{bmatrix}$$\n",
    "\n",
    "- Biases:\n",
    "  $$\\theta=\\begin{bmatrix}-0.4\\\\[4pt]0.2\\\\[4pt]0.1\\end{bmatrix}\\quad(\\theta_1,\\theta_2\\text{ hidden};\\ \\theta_3\\text{ output})$$\n",
    "\n",
    "We use squared error with 1/2:\n",
    "$$E=\\tfrac12 (y-\\hat{y})^2.$$\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step-by-step solution**\n",
    "\n",
    "**1) Hidden pre-activations**\n",
    "$$\n",
    "z = x^\\top W_{\\text{hidden}} + \\begin{bmatrix}\\theta_1\\\\\\theta_2\\end{bmatrix}\n",
    "= \\begin{bmatrix}-0.7\\\\[4pt]0.1\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**2) Hidden activations (ReLU)**\n",
    "$$\n",
    "h = \\max(0,z) = \\begin{bmatrix}0\\\\[4pt]0.1\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**3) Output**\n",
    "$$\n",
    "z_{\\text{out}} = W_{\\text{out}}^\\top h + \\theta_3 = (-0.3)(0) + (-0.2)(0.1) + 0.1 = 0.08\n",
    "$$\n",
    "$$\\hat{y}=0.08$$\n",
    "\n",
    "**4) Error**\n",
    "$$\n",
    "E = \\tfrac12(1-0.08)^2 = \\tfrac12(0.92)^2 = 0.4232\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "**Summary:** hidden inputs $z_1=-0.7,\\; z_2=0.1$; hidden activations $h_1=0,\\; h_2=0.1$; output $\\hat{y}=0.08$; error $E=0.4232$.\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
