{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2943b48",
   "metadata": {},
   "source": [
    "# **Laboratory Task 4**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63337789",
   "metadata": {},
   "source": [
    "**Instruction:** Train a linear regression model in PyTorch using a regression dataset. Use the following parameters.\n",
    "\n",
    "* Criterion: MSE Loss\n",
    "* Fully Connected Layers x 2\n",
    "* Batch Size: 8\n",
    "* Optimizer: SGD\n",
    "* Epoch: 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96bdb335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1eda903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.03807591  0.05068012  0.06169621 ... -0.00259226  0.01990749\n",
      "  -0.01764613]\n",
      " [-0.00188202 -0.04464164 -0.05147406 ... -0.03949338 -0.06833155\n",
      "  -0.09220405]\n",
      " [ 0.08529891  0.05068012  0.04445121 ... -0.00259226  0.00286131\n",
      "  -0.02593034]\n",
      " ...\n",
      " [ 0.04170844  0.05068012 -0.01590626 ... -0.01107952 -0.04688253\n",
      "   0.01549073]\n",
      " [-0.04547248 -0.04464164  0.03906215 ...  0.02655962  0.04452873\n",
      "  -0.02593034]\n",
      " [-0.04547248 -0.04464164 -0.0730303  ... -0.03949338 -0.00422151\n",
      "   0.00306441]] [[151.]\n",
      " [ 75.]\n",
      " [141.]\n",
      " [206.]\n",
      " [135.]\n",
      " [ 97.]\n",
      " [138.]\n",
      " [ 63.]\n",
      " [110.]\n",
      " [310.]\n",
      " [101.]\n",
      " [ 69.]\n",
      " [179.]\n",
      " [185.]\n",
      " [118.]\n",
      " [171.]\n",
      " [166.]\n",
      " [144.]\n",
      " [ 97.]\n",
      " [168.]\n",
      " [ 68.]\n",
      " [ 49.]\n",
      " [ 68.]\n",
      " [245.]\n",
      " [184.]\n",
      " [202.]\n",
      " [137.]\n",
      " [ 85.]\n",
      " [131.]\n",
      " [283.]\n",
      " [129.]\n",
      " [ 59.]\n",
      " [341.]\n",
      " [ 87.]\n",
      " [ 65.]\n",
      " [102.]\n",
      " [265.]\n",
      " [276.]\n",
      " [252.]\n",
      " [ 90.]\n",
      " [100.]\n",
      " [ 55.]\n",
      " [ 61.]\n",
      " [ 92.]\n",
      " [259.]\n",
      " [ 53.]\n",
      " [190.]\n",
      " [142.]\n",
      " [ 75.]\n",
      " [142.]\n",
      " [155.]\n",
      " [225.]\n",
      " [ 59.]\n",
      " [104.]\n",
      " [182.]\n",
      " [128.]\n",
      " [ 52.]\n",
      " [ 37.]\n",
      " [170.]\n",
      " [170.]\n",
      " [ 61.]\n",
      " [144.]\n",
      " [ 52.]\n",
      " [128.]\n",
      " [ 71.]\n",
      " [163.]\n",
      " [150.]\n",
      " [ 97.]\n",
      " [160.]\n",
      " [178.]\n",
      " [ 48.]\n",
      " [270.]\n",
      " [202.]\n",
      " [111.]\n",
      " [ 85.]\n",
      " [ 42.]\n",
      " [170.]\n",
      " [200.]\n",
      " [252.]\n",
      " [113.]\n",
      " [143.]\n",
      " [ 51.]\n",
      " [ 52.]\n",
      " [210.]\n",
      " [ 65.]\n",
      " [141.]\n",
      " [ 55.]\n",
      " [134.]\n",
      " [ 42.]\n",
      " [111.]\n",
      " [ 98.]\n",
      " [164.]\n",
      " [ 48.]\n",
      " [ 96.]\n",
      " [ 90.]\n",
      " [162.]\n",
      " [150.]\n",
      " [279.]\n",
      " [ 92.]\n",
      " [ 83.]\n",
      " [128.]\n",
      " [102.]\n",
      " [302.]\n",
      " [198.]\n",
      " [ 95.]\n",
      " [ 53.]\n",
      " [134.]\n",
      " [144.]\n",
      " [232.]\n",
      " [ 81.]\n",
      " [104.]\n",
      " [ 59.]\n",
      " [246.]\n",
      " [297.]\n",
      " [258.]\n",
      " [229.]\n",
      " [275.]\n",
      " [281.]\n",
      " [179.]\n",
      " [200.]\n",
      " [200.]\n",
      " [173.]\n",
      " [180.]\n",
      " [ 84.]\n",
      " [121.]\n",
      " [161.]\n",
      " [ 99.]\n",
      " [109.]\n",
      " [115.]\n",
      " [268.]\n",
      " [274.]\n",
      " [158.]\n",
      " [107.]\n",
      " [ 83.]\n",
      " [103.]\n",
      " [272.]\n",
      " [ 85.]\n",
      " [280.]\n",
      " [336.]\n",
      " [281.]\n",
      " [118.]\n",
      " [317.]\n",
      " [235.]\n",
      " [ 60.]\n",
      " [174.]\n",
      " [259.]\n",
      " [178.]\n",
      " [128.]\n",
      " [ 96.]\n",
      " [126.]\n",
      " [288.]\n",
      " [ 88.]\n",
      " [292.]\n",
      " [ 71.]\n",
      " [197.]\n",
      " [186.]\n",
      " [ 25.]\n",
      " [ 84.]\n",
      " [ 96.]\n",
      " [195.]\n",
      " [ 53.]\n",
      " [217.]\n",
      " [172.]\n",
      " [131.]\n",
      " [214.]\n",
      " [ 59.]\n",
      " [ 70.]\n",
      " [220.]\n",
      " [268.]\n",
      " [152.]\n",
      " [ 47.]\n",
      " [ 74.]\n",
      " [295.]\n",
      " [101.]\n",
      " [151.]\n",
      " [127.]\n",
      " [237.]\n",
      " [225.]\n",
      " [ 81.]\n",
      " [151.]\n",
      " [107.]\n",
      " [ 64.]\n",
      " [138.]\n",
      " [185.]\n",
      " [265.]\n",
      " [101.]\n",
      " [137.]\n",
      " [143.]\n",
      " [141.]\n",
      " [ 79.]\n",
      " [292.]\n",
      " [178.]\n",
      " [ 91.]\n",
      " [116.]\n",
      " [ 86.]\n",
      " [122.]\n",
      " [ 72.]\n",
      " [129.]\n",
      " [142.]\n",
      " [ 90.]\n",
      " [158.]\n",
      " [ 39.]\n",
      " [196.]\n",
      " [222.]\n",
      " [277.]\n",
      " [ 99.]\n",
      " [196.]\n",
      " [202.]\n",
      " [155.]\n",
      " [ 77.]\n",
      " [191.]\n",
      " [ 70.]\n",
      " [ 73.]\n",
      " [ 49.]\n",
      " [ 65.]\n",
      " [263.]\n",
      " [248.]\n",
      " [296.]\n",
      " [214.]\n",
      " [185.]\n",
      " [ 78.]\n",
      " [ 93.]\n",
      " [252.]\n",
      " [150.]\n",
      " [ 77.]\n",
      " [208.]\n",
      " [ 77.]\n",
      " [108.]\n",
      " [160.]\n",
      " [ 53.]\n",
      " [220.]\n",
      " [154.]\n",
      " [259.]\n",
      " [ 90.]\n",
      " [246.]\n",
      " [124.]\n",
      " [ 67.]\n",
      " [ 72.]\n",
      " [257.]\n",
      " [262.]\n",
      " [275.]\n",
      " [177.]\n",
      " [ 71.]\n",
      " [ 47.]\n",
      " [187.]\n",
      " [125.]\n",
      " [ 78.]\n",
      " [ 51.]\n",
      " [258.]\n",
      " [215.]\n",
      " [303.]\n",
      " [243.]\n",
      " [ 91.]\n",
      " [150.]\n",
      " [310.]\n",
      " [153.]\n",
      " [346.]\n",
      " [ 63.]\n",
      " [ 89.]\n",
      " [ 50.]\n",
      " [ 39.]\n",
      " [103.]\n",
      " [308.]\n",
      " [116.]\n",
      " [145.]\n",
      " [ 74.]\n",
      " [ 45.]\n",
      " [115.]\n",
      " [264.]\n",
      " [ 87.]\n",
      " [202.]\n",
      " [127.]\n",
      " [182.]\n",
      " [241.]\n",
      " [ 66.]\n",
      " [ 94.]\n",
      " [283.]\n",
      " [ 64.]\n",
      " [102.]\n",
      " [200.]\n",
      " [265.]\n",
      " [ 94.]\n",
      " [230.]\n",
      " [181.]\n",
      " [156.]\n",
      " [233.]\n",
      " [ 60.]\n",
      " [219.]\n",
      " [ 80.]\n",
      " [ 68.]\n",
      " [332.]\n",
      " [248.]\n",
      " [ 84.]\n",
      " [200.]\n",
      " [ 55.]\n",
      " [ 85.]\n",
      " [ 89.]\n",
      " [ 31.]\n",
      " [129.]\n",
      " [ 83.]\n",
      " [275.]\n",
      " [ 65.]\n",
      " [198.]\n",
      " [236.]\n",
      " [253.]\n",
      " [124.]\n",
      " [ 44.]\n",
      " [172.]\n",
      " [114.]\n",
      " [142.]\n",
      " [109.]\n",
      " [180.]\n",
      " [144.]\n",
      " [163.]\n",
      " [147.]\n",
      " [ 97.]\n",
      " [220.]\n",
      " [190.]\n",
      " [109.]\n",
      " [191.]\n",
      " [122.]\n",
      " [230.]\n",
      " [242.]\n",
      " [248.]\n",
      " [249.]\n",
      " [192.]\n",
      " [131.]\n",
      " [237.]\n",
      " [ 78.]\n",
      " [135.]\n",
      " [244.]\n",
      " [199.]\n",
      " [270.]\n",
      " [164.]\n",
      " [ 72.]\n",
      " [ 96.]\n",
      " [306.]\n",
      " [ 91.]\n",
      " [214.]\n",
      " [ 95.]\n",
      " [216.]\n",
      " [263.]\n",
      " [178.]\n",
      " [113.]\n",
      " [200.]\n",
      " [139.]\n",
      " [139.]\n",
      " [ 88.]\n",
      " [148.]\n",
      " [ 88.]\n",
      " [243.]\n",
      " [ 71.]\n",
      " [ 77.]\n",
      " [109.]\n",
      " [272.]\n",
      " [ 60.]\n",
      " [ 54.]\n",
      " [221.]\n",
      " [ 90.]\n",
      " [311.]\n",
      " [281.]\n",
      " [182.]\n",
      " [321.]\n",
      " [ 58.]\n",
      " [262.]\n",
      " [206.]\n",
      " [233.]\n",
      " [242.]\n",
      " [123.]\n",
      " [167.]\n",
      " [ 63.]\n",
      " [197.]\n",
      " [ 71.]\n",
      " [168.]\n",
      " [140.]\n",
      " [217.]\n",
      " [121.]\n",
      " [235.]\n",
      " [245.]\n",
      " [ 40.]\n",
      " [ 52.]\n",
      " [104.]\n",
      " [132.]\n",
      " [ 88.]\n",
      " [ 69.]\n",
      " [219.]\n",
      " [ 72.]\n",
      " [201.]\n",
      " [110.]\n",
      " [ 51.]\n",
      " [277.]\n",
      " [ 63.]\n",
      " [118.]\n",
      " [ 69.]\n",
      " [273.]\n",
      " [258.]\n",
      " [ 43.]\n",
      " [198.]\n",
      " [242.]\n",
      " [232.]\n",
      " [175.]\n",
      " [ 93.]\n",
      " [168.]\n",
      " [275.]\n",
      " [293.]\n",
      " [281.]\n",
      " [ 72.]\n",
      " [140.]\n",
      " [189.]\n",
      " [181.]\n",
      " [209.]\n",
      " [136.]\n",
      " [261.]\n",
      " [113.]\n",
      " [131.]\n",
      " [174.]\n",
      " [257.]\n",
      " [ 55.]\n",
      " [ 84.]\n",
      " [ 42.]\n",
      " [146.]\n",
      " [212.]\n",
      " [233.]\n",
      " [ 91.]\n",
      " [111.]\n",
      " [152.]\n",
      " [120.]\n",
      " [ 67.]\n",
      " [310.]\n",
      " [ 94.]\n",
      " [183.]\n",
      " [ 66.]\n",
      " [173.]\n",
      " [ 72.]\n",
      " [ 49.]\n",
      " [ 64.]\n",
      " [ 48.]\n",
      " [178.]\n",
      " [104.]\n",
      " [132.]\n",
      " [220.]\n",
      " [ 57.]]\n"
     ]
    }
   ],
   "source": [
    "# Load dataset (regression dataset: Diabetes)\n",
    "diabetes = load_diabetes()\n",
    "X = diabetes.data\n",
    "y = diabetes.target.reshape(-1, 1)\n",
    "\n",
    "print(X, y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dcae2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.8005,  1.0655,  1.2971,  0.4598, -0.9297, -0.7321, -0.9125, -0.0545,\n",
      "          0.4185, -0.3710],\n",
      "        [-0.0396, -0.9385, -1.0822, -0.5535, -0.1776, -0.4029,  1.5644, -0.8303,\n",
      "         -1.4366, -1.9385],\n",
      "        [ 1.7933,  1.0655,  0.9345, -0.1192, -0.9587, -0.7189, -0.6802, -0.0545,\n",
      "          0.0602, -0.5452],\n",
      "        [-1.8724, -0.9385, -0.2438, -0.7706,  0.2563,  0.5254, -0.7576,  0.7213,\n",
      "          0.4770, -0.1968],\n",
      "        [ 0.1132, -0.9385, -0.7649,  0.4598,  0.0827,  0.3279,  0.1712, -0.0545,\n",
      "         -0.6725, -0.9806]]) tensor([[151.],\n",
      "        [ 75.],\n",
      "        [141.],\n",
      "        [206.],\n",
      "        [135.]])\n"
     ]
    }
   ],
   "source": [
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Convert to tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "print(X_tensor[:5], y_tensor[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a36d163c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create DataLoader\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d426bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model (2 fully connected layers)\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(X.shape[1], 16)  # input -> hidden\n",
    "        self.fc2 = nn.Linear(16, 1)           # hidden -> output\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2659456b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = LinearRegressionModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e0e7bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c87020d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 1144.8914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/1000], Loss: 18771.7520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [300/1000], Loss: 3287.1577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [400/1000], Loss: 465.9929\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     11\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 12\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Print loss every 100 epochs\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\optim\\optimizer.py:493\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    489\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    490\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    491\u001b[0m             )\n\u001b[1;32m--> 493\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    496\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\optim\\optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\optim\\sgd.py:125\u001b[0m, in \u001b[0;36mSGD.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    119\u001b[0m momentum_buffer_list: List[Optional[Tensor]] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    121\u001b[0m has_sparse_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    122\u001b[0m     group, params, grads, momentum_buffer_list\n\u001b[0;32m    123\u001b[0m )\n\u001b[1;32m--> 125\u001b[0m sgd(\n\u001b[0;32m    126\u001b[0m     params,\n\u001b[0;32m    127\u001b[0m     grads,\n\u001b[0;32m    128\u001b[0m     momentum_buffer_list,\n\u001b[0;32m    129\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    130\u001b[0m     momentum\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmomentum\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    131\u001b[0m     lr\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    132\u001b[0m     dampening\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdampening\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    133\u001b[0m     nesterov\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnesterov\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    134\u001b[0m     maximize\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    135\u001b[0m     has_sparse_grad\u001b[38;5;241m=\u001b[39mhas_sparse_grad,\n\u001b[0;32m    136\u001b[0m     foreach\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforeach\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    137\u001b[0m     fused\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    138\u001b[0m     grad_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    139\u001b[0m     found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    140\u001b[0m )\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmomentum\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;66;03m# update momentum_buffers in state\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m p, momentum_buffer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(params, momentum_buffer_list):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\optim\\sgd.py:300\u001b[0m, in \u001b[0;36msgd\u001b[1;34m(params, d_p_list, momentum_buffer_list, has_sparse_grad, foreach, fused, grad_scale, found_inf, weight_decay, momentum, lr, dampening, nesterov, maximize)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    298\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_sgd\n\u001b[1;32m--> 300\u001b[0m func(\n\u001b[0;32m    301\u001b[0m     params,\n\u001b[0;32m    302\u001b[0m     d_p_list,\n\u001b[0;32m    303\u001b[0m     momentum_buffer_list,\n\u001b[0;32m    304\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39mweight_decay,\n\u001b[0;32m    305\u001b[0m     momentum\u001b[38;5;241m=\u001b[39mmomentum,\n\u001b[0;32m    306\u001b[0m     lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[0;32m    307\u001b[0m     dampening\u001b[38;5;241m=\u001b[39mdampening,\n\u001b[0;32m    308\u001b[0m     nesterov\u001b[38;5;241m=\u001b[39mnesterov,\n\u001b[0;32m    309\u001b[0m     has_sparse_grad\u001b[38;5;241m=\u001b[39mhas_sparse_grad,\n\u001b[0;32m    310\u001b[0m     maximize\u001b[38;5;241m=\u001b[39mmaximize,\n\u001b[0;32m    311\u001b[0m     grad_scale\u001b[38;5;241m=\u001b[39mgrad_scale,\n\u001b[0;32m    312\u001b[0m     found_inf\u001b[38;5;241m=\u001b[39mfound_inf,\n\u001b[0;32m    313\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\optim\\sgd.py:353\u001b[0m, in \u001b[0;36m_single_tensor_sgd\u001b[1;34m(params, grads, momentum_buffer_list, grad_scale, found_inf, weight_decay, momentum, lr, dampening, nesterov, maximize, has_sparse_grad)\u001b[0m\n\u001b[0;32m    350\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    351\u001b[0m         grad \u001b[38;5;241m=\u001b[39m buf\n\u001b[1;32m--> 353\u001b[0m param\u001b[38;5;241m.\u001b[39madd_(grad, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mlr)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print loss every 100 epochs\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69731eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Test Loss (MSE): 2920.8279\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test data\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test)\n",
    "    test_loss = criterion(y_pred, y_test).item()\n",
    "\n",
    "print(f\"\\nFinal Test Loss (MSE): {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfc4f45",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "\n",
    "- Training loss shows **large fluctuations** (e.g., ~18 at epoch 300 but >6000 at epoch 500), indicating instability with SGD on small batches.  \n",
    "- Despite fluctuations, the model can occasionally reach **very low training losses (~15)**, meaning it has the capacity to fit the dataset.  \n",
    "- The **final training loss (~14.9)** is much lower than the early epochs, showing the model does learn patterns over time.  \n",
    "- The **final test loss (~2920.8)** is high compared to training loss, suggesting **overfitting** and poor generalization.  \n",
    "- Using the Diabetes dataset (small sample size, noisy target) contributes to unstable convergence.  \n",
    "- Two fully connected layers give the model more flexibility, but with **SGD (no momentum)** and **batch size of 8**, the optimization remains noisy.  \n",
    "- Overall: the model fits training data but struggles to generalize, highlighting dataset limitations and optimizer instability.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cardio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}