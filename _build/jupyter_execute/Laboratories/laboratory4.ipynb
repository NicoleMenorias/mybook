{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2943b48",
   "metadata": {},
   "source": [
    "# **Laboratory Task 4**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63337789",
   "metadata": {},
   "source": [
    "**Instruction:** Train a linear regression model in PyTorch using a regression dataset. Use the following parameters.\n",
    "\n",
    "* Criterion: MSE Loss\n",
    "* Fully Connected Layers x 2\n",
    "* Batch Size: 8\n",
    "* Optimizer: SGD\n",
    "* Epoch: 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96bdb335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1eda903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.03807591  0.05068012  0.06169621 ... -0.00259226  0.01990749\n",
      "  -0.01764613]\n",
      " [-0.00188202 -0.04464164 -0.05147406 ... -0.03949338 -0.06833155\n",
      "  -0.09220405]\n",
      " [ 0.08529891  0.05068012  0.04445121 ... -0.00259226  0.00286131\n",
      "  -0.02593034]\n",
      " ...\n",
      " [ 0.04170844  0.05068012 -0.01590626 ... -0.01107952 -0.04688253\n",
      "   0.01549073]\n",
      " [-0.04547248 -0.04464164  0.03906215 ...  0.02655962  0.04452873\n",
      "  -0.02593034]\n",
      " [-0.04547248 -0.04464164 -0.0730303  ... -0.03949338 -0.00422151\n",
      "   0.00306441]] [[151.]\n",
      " [ 75.]\n",
      " [141.]\n",
      " [206.]\n",
      " [135.]\n",
      " [ 97.]\n",
      " [138.]\n",
      " [ 63.]\n",
      " [110.]\n",
      " [310.]\n",
      " [101.]\n",
      " [ 69.]\n",
      " [179.]\n",
      " [185.]\n",
      " [118.]\n",
      " [171.]\n",
      " [166.]\n",
      " [144.]\n",
      " [ 97.]\n",
      " [168.]\n",
      " [ 68.]\n",
      " [ 49.]\n",
      " [ 68.]\n",
      " [245.]\n",
      " [184.]\n",
      " [202.]\n",
      " [137.]\n",
      " [ 85.]\n",
      " [131.]\n",
      " [283.]\n",
      " [129.]\n",
      " [ 59.]\n",
      " [341.]\n",
      " [ 87.]\n",
      " [ 65.]\n",
      " [102.]\n",
      " [265.]\n",
      " [276.]\n",
      " [252.]\n",
      " [ 90.]\n",
      " [100.]\n",
      " [ 55.]\n",
      " [ 61.]\n",
      " [ 92.]\n",
      " [259.]\n",
      " [ 53.]\n",
      " [190.]\n",
      " [142.]\n",
      " [ 75.]\n",
      " [142.]\n",
      " [155.]\n",
      " [225.]\n",
      " [ 59.]\n",
      " [104.]\n",
      " [182.]\n",
      " [128.]\n",
      " [ 52.]\n",
      " [ 37.]\n",
      " [170.]\n",
      " [170.]\n",
      " [ 61.]\n",
      " [144.]\n",
      " [ 52.]\n",
      " [128.]\n",
      " [ 71.]\n",
      " [163.]\n",
      " [150.]\n",
      " [ 97.]\n",
      " [160.]\n",
      " [178.]\n",
      " [ 48.]\n",
      " [270.]\n",
      " [202.]\n",
      " [111.]\n",
      " [ 85.]\n",
      " [ 42.]\n",
      " [170.]\n",
      " [200.]\n",
      " [252.]\n",
      " [113.]\n",
      " [143.]\n",
      " [ 51.]\n",
      " [ 52.]\n",
      " [210.]\n",
      " [ 65.]\n",
      " [141.]\n",
      " [ 55.]\n",
      " [134.]\n",
      " [ 42.]\n",
      " [111.]\n",
      " [ 98.]\n",
      " [164.]\n",
      " [ 48.]\n",
      " [ 96.]\n",
      " [ 90.]\n",
      " [162.]\n",
      " [150.]\n",
      " [279.]\n",
      " [ 92.]\n",
      " [ 83.]\n",
      " [128.]\n",
      " [102.]\n",
      " [302.]\n",
      " [198.]\n",
      " [ 95.]\n",
      " [ 53.]\n",
      " [134.]\n",
      " [144.]\n",
      " [232.]\n",
      " [ 81.]\n",
      " [104.]\n",
      " [ 59.]\n",
      " [246.]\n",
      " [297.]\n",
      " [258.]\n",
      " [229.]\n",
      " [275.]\n",
      " [281.]\n",
      " [179.]\n",
      " [200.]\n",
      " [200.]\n",
      " [173.]\n",
      " [180.]\n",
      " [ 84.]\n",
      " [121.]\n",
      " [161.]\n",
      " [ 99.]\n",
      " [109.]\n",
      " [115.]\n",
      " [268.]\n",
      " [274.]\n",
      " [158.]\n",
      " [107.]\n",
      " [ 83.]\n",
      " [103.]\n",
      " [272.]\n",
      " [ 85.]\n",
      " [280.]\n",
      " [336.]\n",
      " [281.]\n",
      " [118.]\n",
      " [317.]\n",
      " [235.]\n",
      " [ 60.]\n",
      " [174.]\n",
      " [259.]\n",
      " [178.]\n",
      " [128.]\n",
      " [ 96.]\n",
      " [126.]\n",
      " [288.]\n",
      " [ 88.]\n",
      " [292.]\n",
      " [ 71.]\n",
      " [197.]\n",
      " [186.]\n",
      " [ 25.]\n",
      " [ 84.]\n",
      " [ 96.]\n",
      " [195.]\n",
      " [ 53.]\n",
      " [217.]\n",
      " [172.]\n",
      " [131.]\n",
      " [214.]\n",
      " [ 59.]\n",
      " [ 70.]\n",
      " [220.]\n",
      " [268.]\n",
      " [152.]\n",
      " [ 47.]\n",
      " [ 74.]\n",
      " [295.]\n",
      " [101.]\n",
      " [151.]\n",
      " [127.]\n",
      " [237.]\n",
      " [225.]\n",
      " [ 81.]\n",
      " [151.]\n",
      " [107.]\n",
      " [ 64.]\n",
      " [138.]\n",
      " [185.]\n",
      " [265.]\n",
      " [101.]\n",
      " [137.]\n",
      " [143.]\n",
      " [141.]\n",
      " [ 79.]\n",
      " [292.]\n",
      " [178.]\n",
      " [ 91.]\n",
      " [116.]\n",
      " [ 86.]\n",
      " [122.]\n",
      " [ 72.]\n",
      " [129.]\n",
      " [142.]\n",
      " [ 90.]\n",
      " [158.]\n",
      " [ 39.]\n",
      " [196.]\n",
      " [222.]\n",
      " [277.]\n",
      " [ 99.]\n",
      " [196.]\n",
      " [202.]\n",
      " [155.]\n",
      " [ 77.]\n",
      " [191.]\n",
      " [ 70.]\n",
      " [ 73.]\n",
      " [ 49.]\n",
      " [ 65.]\n",
      " [263.]\n",
      " [248.]\n",
      " [296.]\n",
      " [214.]\n",
      " [185.]\n",
      " [ 78.]\n",
      " [ 93.]\n",
      " [252.]\n",
      " [150.]\n",
      " [ 77.]\n",
      " [208.]\n",
      " [ 77.]\n",
      " [108.]\n",
      " [160.]\n",
      " [ 53.]\n",
      " [220.]\n",
      " [154.]\n",
      " [259.]\n",
      " [ 90.]\n",
      " [246.]\n",
      " [124.]\n",
      " [ 67.]\n",
      " [ 72.]\n",
      " [257.]\n",
      " [262.]\n",
      " [275.]\n",
      " [177.]\n",
      " [ 71.]\n",
      " [ 47.]\n",
      " [187.]\n",
      " [125.]\n",
      " [ 78.]\n",
      " [ 51.]\n",
      " [258.]\n",
      " [215.]\n",
      " [303.]\n",
      " [243.]\n",
      " [ 91.]\n",
      " [150.]\n",
      " [310.]\n",
      " [153.]\n",
      " [346.]\n",
      " [ 63.]\n",
      " [ 89.]\n",
      " [ 50.]\n",
      " [ 39.]\n",
      " [103.]\n",
      " [308.]\n",
      " [116.]\n",
      " [145.]\n",
      " [ 74.]\n",
      " [ 45.]\n",
      " [115.]\n",
      " [264.]\n",
      " [ 87.]\n",
      " [202.]\n",
      " [127.]\n",
      " [182.]\n",
      " [241.]\n",
      " [ 66.]\n",
      " [ 94.]\n",
      " [283.]\n",
      " [ 64.]\n",
      " [102.]\n",
      " [200.]\n",
      " [265.]\n",
      " [ 94.]\n",
      " [230.]\n",
      " [181.]\n",
      " [156.]\n",
      " [233.]\n",
      " [ 60.]\n",
      " [219.]\n",
      " [ 80.]\n",
      " [ 68.]\n",
      " [332.]\n",
      " [248.]\n",
      " [ 84.]\n",
      " [200.]\n",
      " [ 55.]\n",
      " [ 85.]\n",
      " [ 89.]\n",
      " [ 31.]\n",
      " [129.]\n",
      " [ 83.]\n",
      " [275.]\n",
      " [ 65.]\n",
      " [198.]\n",
      " [236.]\n",
      " [253.]\n",
      " [124.]\n",
      " [ 44.]\n",
      " [172.]\n",
      " [114.]\n",
      " [142.]\n",
      " [109.]\n",
      " [180.]\n",
      " [144.]\n",
      " [163.]\n",
      " [147.]\n",
      " [ 97.]\n",
      " [220.]\n",
      " [190.]\n",
      " [109.]\n",
      " [191.]\n",
      " [122.]\n",
      " [230.]\n",
      " [242.]\n",
      " [248.]\n",
      " [249.]\n",
      " [192.]\n",
      " [131.]\n",
      " [237.]\n",
      " [ 78.]\n",
      " [135.]\n",
      " [244.]\n",
      " [199.]\n",
      " [270.]\n",
      " [164.]\n",
      " [ 72.]\n",
      " [ 96.]\n",
      " [306.]\n",
      " [ 91.]\n",
      " [214.]\n",
      " [ 95.]\n",
      " [216.]\n",
      " [263.]\n",
      " [178.]\n",
      " [113.]\n",
      " [200.]\n",
      " [139.]\n",
      " [139.]\n",
      " [ 88.]\n",
      " [148.]\n",
      " [ 88.]\n",
      " [243.]\n",
      " [ 71.]\n",
      " [ 77.]\n",
      " [109.]\n",
      " [272.]\n",
      " [ 60.]\n",
      " [ 54.]\n",
      " [221.]\n",
      " [ 90.]\n",
      " [311.]\n",
      " [281.]\n",
      " [182.]\n",
      " [321.]\n",
      " [ 58.]\n",
      " [262.]\n",
      " [206.]\n",
      " [233.]\n",
      " [242.]\n",
      " [123.]\n",
      " [167.]\n",
      " [ 63.]\n",
      " [197.]\n",
      " [ 71.]\n",
      " [168.]\n",
      " [140.]\n",
      " [217.]\n",
      " [121.]\n",
      " [235.]\n",
      " [245.]\n",
      " [ 40.]\n",
      " [ 52.]\n",
      " [104.]\n",
      " [132.]\n",
      " [ 88.]\n",
      " [ 69.]\n",
      " [219.]\n",
      " [ 72.]\n",
      " [201.]\n",
      " [110.]\n",
      " [ 51.]\n",
      " [277.]\n",
      " [ 63.]\n",
      " [118.]\n",
      " [ 69.]\n",
      " [273.]\n",
      " [258.]\n",
      " [ 43.]\n",
      " [198.]\n",
      " [242.]\n",
      " [232.]\n",
      " [175.]\n",
      " [ 93.]\n",
      " [168.]\n",
      " [275.]\n",
      " [293.]\n",
      " [281.]\n",
      " [ 72.]\n",
      " [140.]\n",
      " [189.]\n",
      " [181.]\n",
      " [209.]\n",
      " [136.]\n",
      " [261.]\n",
      " [113.]\n",
      " [131.]\n",
      " [174.]\n",
      " [257.]\n",
      " [ 55.]\n",
      " [ 84.]\n",
      " [ 42.]\n",
      " [146.]\n",
      " [212.]\n",
      " [233.]\n",
      " [ 91.]\n",
      " [111.]\n",
      " [152.]\n",
      " [120.]\n",
      " [ 67.]\n",
      " [310.]\n",
      " [ 94.]\n",
      " [183.]\n",
      " [ 66.]\n",
      " [173.]\n",
      " [ 72.]\n",
      " [ 49.]\n",
      " [ 64.]\n",
      " [ 48.]\n",
      " [178.]\n",
      " [104.]\n",
      " [132.]\n",
      " [220.]\n",
      " [ 57.]]\n"
     ]
    }
   ],
   "source": [
    "# Load dataset (regression dataset: Diabetes)\n",
    "diabetes = load_diabetes()\n",
    "X = diabetes.data\n",
    "y = diabetes.target.reshape(-1, 1)\n",
    "\n",
    "print(X, y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dcae2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.8005,  1.0655,  1.2971,  0.4598, -0.9297, -0.7321, -0.9125, -0.0545,\n",
      "          0.4185, -0.3710],\n",
      "        [-0.0396, -0.9385, -1.0822, -0.5535, -0.1776, -0.4029,  1.5644, -0.8303,\n",
      "         -1.4366, -1.9385],\n",
      "        [ 1.7933,  1.0655,  0.9345, -0.1192, -0.9587, -0.7189, -0.6802, -0.0545,\n",
      "          0.0602, -0.5452],\n",
      "        [-1.8724, -0.9385, -0.2438, -0.7706,  0.2563,  0.5254, -0.7576,  0.7213,\n",
      "          0.4770, -0.1968],\n",
      "        [ 0.1132, -0.9385, -0.7649,  0.4598,  0.0827,  0.3279,  0.1712, -0.0545,\n",
      "         -0.6725, -0.9806]]) tensor([[151.],\n",
      "        [ 75.],\n",
      "        [141.],\n",
      "        [206.],\n",
      "        [135.]])\n"
     ]
    }
   ],
   "source": [
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Convert to tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "print(X_tensor[:5], y_tensor[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a36d163c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create DataLoader\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d426bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model (2 fully connected layers)\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(X.shape[1], 16)  # input -> hidden\n",
    "        self.fc2 = nn.Linear(16, 1)           # hidden -> output\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2659456b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = LinearRegressionModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e0e7bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c87020d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 176.4633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/1000], Loss: 5611.1294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [300/1000], Loss: 16659.4062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [400/1000], Loss: 250.5667\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_X, batch_y \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m      6\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(batch_X)\n\u001b[0;32m      7\u001b[0m         loss \u001b[38;5;241m=\u001b[39m criterion(outputs, batch_y)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:704\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m--> 704\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_name):\n\u001b[0;32m    705\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m             \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    707\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\autograd\\profiler.py:757\u001b[0m, in \u001b[0;36mrecord_function.__exit__\u001b[1;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[0;32m    752\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecord \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39m_record_function_enter_new(\n\u001b[0;32m    753\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\n\u001b[0;32m    754\u001b[0m     )\n\u001b[0;32m    755\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m--> 757\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type: Any, exc_value: Any, traceback: Any):\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_callbacks_on_exit:\n\u001b[0;32m    759\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print loss every 100 epochs\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69731eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Test Loss (MSE): 2920.8279\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test data\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test)\n",
    "    test_loss = criterion(y_pred, y_test).item()\n",
    "\n",
    "print(f\"\\nFinal Test Loss (MSE): {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfc4f45",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "\n",
    "- Training loss shows **large fluctuations** (e.g., ~18 at epoch 300 but >6000 at epoch 500), indicating instability with SGD on small batches.  \n",
    "- Despite fluctuations, the model can occasionally reach **very low training losses (~15)**, meaning it has the capacity to fit the dataset.  \n",
    "- The **final training loss (~14.9)** is much lower than the early epochs, showing the model does learn patterns over time.  \n",
    "- The **final test loss (~2920.8)** is high compared to training loss, suggesting **overfitting** and poor generalization.  \n",
    "- Using the Diabetes dataset (small sample size, noisy target) contributes to unstable convergence.  \n",
    "- Two fully connected layers give the model more flexibility, but with **SGD (no momentum)** and **batch size of 8**, the optimization remains noisy.  \n",
    "- Overall: the model fits training data but struggles to generalize, highlighting dataset limitations and optimizer instability.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cardio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}